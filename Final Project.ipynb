{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O949bBTz2RnR"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "executionInfo": {
     "elapsed": 2175,
     "status": "ok",
     "timestamp": 1754564723760,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "KIE3DXDp109Q"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# from google.cloud import bigquery\n",
    "# from google.colab import data_table\n",
    "# from google.colab import widgets\n",
    "\n",
    "# from collections import Counter\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22629,
     "status": "ok",
     "timestamp": 1754564746394,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "oUeMEbyU2Uga",
    "outputId": "8402b305-b170-4709-eae5-ddb9cc1eaad1"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "# import sys\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# os.chdir('/content/drive/MyDrive/HML/Final Project')\n",
    "# sys.path.append(os.path.abspath('/content/drive/MyDrive/HML/Final Project'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "executionInfo": {
     "elapsed": 6357,
     "status": "ok",
     "timestamp": 1754564787170,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "z2DeVlHnNdm2"
   },
   "outputs": [],
   "source": [
    "# !pip install duckdb --quiet\n",
    "# import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "executionInfo": {
     "elapsed": 4092,
     "status": "ok",
     "timestamp": 1754564841428,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "ZSLwVEZTNX5S"
   },
   "outputs": [],
   "source": [
    "# drive_path = '/content/drive/MyDrive/HML/Final Project/MIMIC-III'\n",
    "# con = duckdb.connect(f'{drive_path}/mimiciii.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bnet/kupershmidt/anaconda3/envs/spider/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-17 18:05:10.366255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755443110.380432  789779 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755443110.384486  789779 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755443110.396053  789779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755443110.396064  789779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755443110.396066  789779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755443110.396067  789779 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-17 18:05:10.399938: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18obiL-i3Up1"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2434,
     "status": "ok",
     "timestamp": 1754564996788,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "1IsleQzR3RVc"
   },
   "outputs": [],
   "source": [
    "subject_ids = pd.read_csv('data/initial_cohort.csv')['subject_id'].to_list()\n",
    "lavbevent_meatdata = pd.read_csv('data/labs_metadata.csv')\n",
    "vital_meatdata = pd.read_csv('data/vital_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "befffb4f7372406291ce48f9bfcdb07b",
      "56463d5b8bee4a088d7c9d7613fd6e20",
      "da6f1034c8da4045a1b7aee2d5f78d5d"
     ]
    },
    "executionInfo": {
     "elapsed": 5786,
     "status": "ok",
     "timestamp": 1754564888143,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "AS2L1U3z3RSs",
    "outputId": "3d491053-ad59-4cbb-e4bf-c4e6b8a91188"
   },
   "outputs": [],
   "source": [
    "ICUQ = \\\n",
    "\"\"\"--sql\n",
    "SELECT admissions.subject_id::INTEGER AS subject_id, admissions.hadm_id::INTEGER AS hadm_id\n",
    ", admissions.admittime::DATE AS admittime, admissions.dischtime::DATE AS dischtime\n",
    ", admissions.ethnicity, admissions.deathtime::DATE AS deathtime\n",
    ", patients.gender, patients.dob::DATE AS dob, icustays.icustay_id::INTEGER AS icustay_id, patients.dod::DATE as dod,\n",
    "icustays.intime::DATE AS intime,icustays.outtime::DATE AS outtime\n",
    "FROM admissions\n",
    "INNER JOIN patients\n",
    "    ON admissions.subject_id = patients.subject_id\n",
    "LEFT JOIN icustays\n",
    "    ON admissions.hadm_id = icustays.hadm_id\n",
    "\n",
    "WHERE admissions.has_chartevents_data = 1\n",
    "AND admissions.subject_id::INTEGER IN ?\n",
    "ORDER BY admissions.subject_id, admissions.hadm_id, admissions.admittime;\n",
    "\"\"\"\n",
    "\n",
    "# icu =  con.execute(ICUQ, [subject_ids]).fetchdf().rename(str.lower, axis='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79,
     "referenced_widgets": [
      "e6c51c60ee844703a0339b2e1dccc64b",
      "c82cd5cfb39e42af888dc187b5d8a58c",
      "c94eb62fe73b4171937f354e2c2f06fa",
      "78c007406aa24a9f80d79038bb6e14a1",
      "fb91ad428b04480f995aced9d968ba18",
      "1ddf7f8e5bd9405fb67d08725d4983e5"
     ]
    },
    "executionInfo": {
     "elapsed": 315317,
     "status": "ok",
     "timestamp": 1754565343251,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "JYcG1ZyK3ROu",
    "outputId": "48833e31-a0af-4ee5-ed98-abe2f2aadbff"
   },
   "outputs": [],
   "source": [
    "LABQUERY = \\\n",
    "f\"\"\"--sql\n",
    "SELECT labevents.subject_id::INTEGER AS subject_id\\\n",
    "      , labevents.hadm_id::INTEGER AS hadm_id\\\n",
    "      , labevents.charttime::DATE AS charttime\n",
    "      , labevents.itemid::INTEGER AS itemid\\\n",
    "      , labevents.valuenum::DOUBLE AS valuenum\n",
    "      , admissions.admittime::DATE AS admittime\n",
    "FROM labevents\n",
    "          INNER JOIN admissions\n",
    "                    ON labevents.subject_id = admissions.subject_id\n",
    "                        AND labevents.hadm_id = admissions.hadm_id\n",
    "                        AND labevents.charttime::DATE between\n",
    "                            (admissions.admittime::DATE)\n",
    "                            AND (admissions.admittime::DATE + interval 48 hour)\n",
    "                        AND itemid::INTEGER IN ? \\\n",
    "                        \"\"\"\n",
    "\n",
    "VITQUERY = f\"\"\"--sql\n",
    "        SELECT chartevents.subject_id::INTEGER AS subject_id\\\n",
    "             , chartevents.hadm_id::INTEGER AS hadm_id\\\n",
    "             , chartevents.charttime::DATE AS charttime\\\n",
    "             , chartevents.itemid::INTEGER AS itemid\\\n",
    "             , chartevents.valuenum::DOUBLE AS valuenum\\\n",
    "             , admissions.admittime::DATE AS admittime\\\n",
    "        FROM chartevents\n",
    "                 INNER JOIN admissions\n",
    "                            ON chartevents.subject_id = admissions.subject_id\n",
    "                                AND chartevents.hadm_id = admissions.hadm_id\n",
    "                                AND chartevents.charttime::DATE between\n",
    "                                   (admissions.admittime::DATE)\n",
    "                                   AND (admissions.admittime::DATE + interval 48 hour)\n",
    "                                AND itemid::INTEGER in ?\n",
    "      -- exclude rows marked as error\n",
    "      AND chartevents.error::INTEGER IS DISTINCT \\\n",
    "        FROM 1 \\\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "# lab = con.execute(LABQUERY, [lavbevent_meatdata['itemid'].tolist()]).fetchdf().rename(str.lower, axis='columns')\n",
    "# vit = con.execute(VITQUERY, [vital_meatdata['itemid'].tolist()]).fetchdf().rename(str.lower, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1754566848823,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "63QSy3WiOcK7"
   },
   "outputs": [],
   "source": [
    "pred_window = 13*24      # duration of prediction window (hours)\n",
    "pred_gap = 24            # minimal gap between prediction and target (hours)\n",
    "min_los = 24             # minimal length of stay (hours)\n",
    "min_target_onset = 2*24  # minimal time of target since admission (hours)\n",
    "pred_freq = '4H'        # prediction frequency\n",
    "\n",
    "labs = pd.read_csv('data/labs.csv')\n",
    "vits = pd.read_csv('data/vits.csv')\n",
    "hosps = pd.read_csv('data/icu.csv')\n",
    "\n",
    "for col in ['admittime', 'dischtime', 'dob', 'dod', 'intime', 'outtime']:\n",
    "    hosps[col] = pd.to_datetime(hosps[col].str.strip(), errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ethnicity  - to category\n",
    "hosps.ethnicity = hosps.ethnicity.str.lower()\n",
    "hosps.loc[(hosps.ethnicity.str.contains('^white')),'ethnicity'] = 'white'\n",
    "hosps.loc[(hosps.ethnicity.str.contains('^black')),'ethnicity'] = 'black'\n",
    "hosps.loc[(hosps.ethnicity.str.contains('^hisp')) | (hosps.ethnicity.str.contains('^latin')),'ethnicity'] = 'hispanic'\n",
    "hosps.loc[(hosps.ethnicity.str.contains('^asia')),'ethnicity'] = 'asian'\n",
    "hosps.loc[~(hosps.ethnicity.str.contains('|'.join(['white', 'black', 'hispanic', 'asian']))),'ethnicity'] = 'other'\n",
    "\n",
    "# ethnicity - one hot encoding\n",
    "hosps['eth_white'] = (hosps['ethnicity'] == 'white').astype(int)\n",
    "hosps['eth_black'] = (hosps['ethnicity'] == 'black').astype(int)\n",
    "hosps['eth_hispanic'] = (hosps['ethnicity'] == 'hispanic').astype(int)\n",
    "hosps['eth_asian'] = (hosps['ethnicity'] == 'asian').astype(int)\n",
    "hosps['eth_other'] = (hosps['ethnicity'] == 'other').astype(int)\n",
    "hosps.drop(['ethnicity', 'deathtime'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 305,
     "status": "error",
     "timestamp": 1754566885343,
     "user": {
      "displayName": "Yael Kupershmidt",
      "userId": "00404238359436170384"
     },
     "user_tz": -180
    },
    "id": "ynUWK92_VQdK",
    "outputId": "b0a3f0ea-1e36-48d3-e6ce-b584be199fdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Include only first admissions: N=32513\n",
      "2. Exclusion by ages: N=25548\n",
      "3. Include only patients who admitted for at least 24 hours: N=25168\n",
      "4. Exclude patients who died within 48-hours of admission: N=24825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate feature columns for los, age and mortality\n",
    "def age(admittime, dob):\n",
    "    if admittime < dob:\n",
    "      return 0\n",
    "    return admittime.year - dob.year - ((admittime.month, admittime.day) < (dob.month, dob.day))\n",
    "\n",
    "hosps['age'] = hosps.apply(lambda row: age(row['admittime'], row['dob']), axis=1)\n",
    "hosps['los_hosp_hr'] = (hosps.dischtime - hosps.admittime).dt.total_seconds()/3600\n",
    "hosps['mort'] = np.where(~np.isnat(hosps.dod),1,0)\n",
    "\n",
    "# Gender to binary\n",
    "hosps['gender'] = np.where(hosps['gender']==\"M\", 1, 0)\n",
    "\n",
    "# @title Q1.1 - Patient Exclusion Criteria\n",
    "hosps = hosps.sort_values('admittime').groupby('subject_id').first().reset_index()\n",
    "print(f\"1. Include only first admissions: N={hosps.shape[0]}\")\n",
    "\n",
    "hosps = hosps[hosps.age.between(18,90)]\n",
    "print(f\"2. Exclusion by ages: N={hosps.shape[0]}\")\n",
    "\n",
    "# Exclude patients hospitalized for less than 24 hours\n",
    "hosps = hosps[hosps['los_hosp_hr'] >= min_los]\n",
    "print(f\"3. Include only patients who admitted for at least {min_los} hours: N={hosps.shape[0]}\")\n",
    "\n",
    "# Exclude patients that died in the first 48 hours of admission\n",
    "hours_to_death = (hosps['dod'] - hosps['admittime']).dt.total_seconds() / 3600\n",
    "hosps = hosps[~((hosps['mort'].astype(bool)) & (hours_to_death < min_target_onset))]\n",
    "print(f\"4. Exclude patients who died within {min_target_onset}-hours of admission: N={hosps.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_789779/9803228.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  pivot = pd.pivot_table(merged, index=['subject_id', 'hadm_id', pd.Grouper(key='charttime', freq=pred_freq)],\n"
     ]
    }
   ],
   "source": [
    "labs = labs[labs['hadm_id'].isin(hosps['hadm_id'])]\n",
    "labs = pd.merge(labs,lavbevent_meatdata,on='itemid')\n",
    "labs = labs[labs['valuenum'].between(labs['min'],labs['max'],  inclusive='both')]\n",
    "\n",
    "vits = vits[vits['hadm_id'].isin(hosps['hadm_id'])]\n",
    "vits = pd.merge(vits,vital_meatdata,on='itemid')\n",
    "vits = vits[vits['valuenum'].between(vits['min'],vits['max'], inclusive='both')]\n",
    "\n",
    "vits.loc[(vits['feature name'] == 'TempF'),'valuenum'] = (vits[vits['feature name'] == 'TempF']['valuenum']-32)/1.8\n",
    "vits.loc[vits['feature name'] == 'TempF','feature name'] = 'TempC'\n",
    "\n",
    "merged = pd.concat([vits, labs])\n",
    "merged['charttime'] = pd.to_datetime(merged['charttime'], errors='coerce')\n",
    "\n",
    "pivot = pd.pivot_table(merged, index=['subject_id', 'hadm_id', pd.Grouper(key='charttime', freq=pred_freq)],\n",
    "                       columns=['feature name'], values='valuenum', aggfunc=['mean', 'max', 'min', 'std'])\n",
    "pivot.columns = [f'{c[1]}_{c[0]}' for c in pivot.columns.to_flat_index()]\n",
    "\n",
    "# temp = merged.copy()\n",
    "\n",
    "merged = pd.merge(hosps, pivot.reset_index(), on=['subject_id', 'hadm_id'])\n",
    "merged[pivot.columns] = merged.groupby(['subject_id', 'hadm_id'])[pivot.columns].ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "subject_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hadm_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "charttime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "itemid",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "valuenum",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "admittime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "measurement name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "feature name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "min",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "max",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "units",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7ec249a9-d1ec-439d-944f-9edc4649f5b3",
       "rows": [
        [
         "0",
         "12215",
         "266",
         "186251",
         "2168-07-11 14:00:00",
         "220179",
         "132.0",
         "2168-07-10",
         " Non Invasive Blood Pressure systolic",
         "SysBP",
         "20",
         "250",
         "mmHg"
        ],
        [
         "1",
         "12216",
         "266",
         "186251",
         "2168-07-11 14:00:00",
         "220180",
         "78.0",
         "2168-07-10",
         " Non Invasive Blood Pressure diastolic",
         "DiasBP",
         "20",
         "240",
         "mmHg"
        ],
        [
         "2",
         "12217",
         "266",
         "186251",
         "2168-07-11 14:00:00",
         "220181",
         "89.0",
         "2168-07-10",
         " Non Invasive Blood Pressure mean",
         "MeanBP",
         "20",
         "245",
         "mmHg"
        ],
        [
         "3",
         "12218",
         "266",
         "186251",
         "2168-07-11 14:00:00",
         "220210",
         "17.0",
         "2168-07-10",
         " Respiratory Rate",
         "RespRate",
         "5",
         "100",
         "BPM"
        ],
        [
         "4",
         "12219",
         "266",
         "186251",
         "2168-07-11 14:00:00",
         "220277",
         "99.0",
         "2168-07-10",
         " peripheral",
         "SpO2",
         "5",
         "100",
         "%"
        ],
        [
         "5",
         "12220",
         "266",
         "186251",
         "2168-07-11 14:00:00",
         "225664",
         "108.0",
         "2168-07-10",
         " Glucose finger stick",
         "Glucose",
         "15",
         "2000",
         "mg/dL"
        ],
        [
         "6",
         "12221",
         "266",
         "186251",
         "2168-07-11 15:00:00",
         "220045",
         "75.0",
         "2168-07-10",
         " Heart Rate",
         "HeartRate",
         "10",
         "300",
         "BPM"
        ],
        [
         "7",
         "12222",
         "266",
         "186251",
         "2168-07-11 15:00:00",
         "220179",
         "134.0",
         "2168-07-10",
         " Non Invasive Blood Pressure systolic",
         "SysBP",
         "20",
         "250",
         "mmHg"
        ],
        [
         "8",
         "12223",
         "266",
         "186251",
         "2168-07-11 15:00:00",
         "220180",
         "71.0",
         "2168-07-10",
         " Non Invasive Blood Pressure diastolic",
         "DiasBP",
         "20",
         "240",
         "mmHg"
        ],
        [
         "9",
         "12224",
         "266",
         "186251",
         "2168-07-11 15:00:00",
         "220181",
         "86.0",
         "2168-07-10",
         " Non Invasive Blood Pressure mean",
         "MeanBP",
         "20",
         "245",
         "mmHg"
        ],
        [
         "10",
         "12225",
         "266",
         "186251",
         "2168-07-11 15:00:00",
         "220210",
         "18.0",
         "2168-07-10",
         " Respiratory Rate",
         "RespRate",
         "5",
         "100",
         "BPM"
        ],
        [
         "11",
         "12226",
         "266",
         "186251",
         "2168-07-11 15:00:00",
         "220277",
         "99.0",
         "2168-07-10",
         " peripheral",
         "SpO2",
         "5",
         "100",
         "%"
        ],
        [
         "12",
         "12227",
         "266",
         "186251",
         "2168-07-11 16:00:00",
         "220045",
         "78.0",
         "2168-07-10",
         " Heart Rate",
         "HeartRate",
         "10",
         "300",
         "BPM"
        ],
        [
         "13",
         "12228",
         "266",
         "186251",
         "2168-07-11 16:00:00",
         "220179",
         "133.0",
         "2168-07-10",
         " Non Invasive Blood Pressure systolic",
         "SysBP",
         "20",
         "250",
         "mmHg"
        ],
        [
         "14",
         "12229",
         "266",
         "186251",
         "2168-07-11 16:00:00",
         "220180",
         "72.0",
         "2168-07-10",
         " Non Invasive Blood Pressure diastolic",
         "DiasBP",
         "20",
         "240",
         "mmHg"
        ],
        [
         "15",
         "12230",
         "266",
         "186251",
         "2168-07-11 16:00:00",
         "220181",
         "86.0",
         "2168-07-10",
         " Non Invasive Blood Pressure mean",
         "MeanBP",
         "20",
         "245",
         "mmHg"
        ],
        [
         "16",
         "12231",
         "266",
         "186251",
         "2168-07-11 16:00:00",
         "220210",
         "18.0",
         "2168-07-10",
         " Respiratory Rate",
         "RespRate",
         "5",
         "100",
         "BPM"
        ],
        [
         "17",
         "12232",
         "266",
         "186251",
         "2168-07-11 16:00:00",
         "220277",
         "100.0",
         "2168-07-10",
         " peripheral",
         "SpO2",
         "5",
         "100",
         "%"
        ],
        [
         "18",
         "12233",
         "266",
         "186251",
         "2168-07-11 16:00:00",
         "223761",
         "36.83333333333333",
         "2168-07-10",
         " Temperature Fahrenheit",
         "TempC",
         "68",
         "113",
         "F"
        ],
        [
         "19",
         "12234",
         "266",
         "186251",
         "2168-07-11 17:00:00",
         "220045",
         "87.0",
         "2168-07-10",
         " Heart Rate",
         "HeartRate",
         "10",
         "300",
         "BPM"
        ],
        [
         "20",
         "12235",
         "266",
         "186251",
         "2168-07-11 17:00:00",
         "220179",
         "145.0",
         "2168-07-10",
         " Non Invasive Blood Pressure systolic",
         "SysBP",
         "20",
         "250",
         "mmHg"
        ],
        [
         "21",
         "12236",
         "266",
         "186251",
         "2168-07-11 17:00:00",
         "220180",
         "73.0",
         "2168-07-10",
         " Non Invasive Blood Pressure diastolic",
         "DiasBP",
         "20",
         "240",
         "mmHg"
        ],
        [
         "22",
         "12237",
         "266",
         "186251",
         "2168-07-11 17:00:00",
         "220181",
         "91.0",
         "2168-07-10",
         " Non Invasive Blood Pressure mean",
         "MeanBP",
         "20",
         "245",
         "mmHg"
        ],
        [
         "23",
         "12238",
         "266",
         "186251",
         "2168-07-11 17:00:00",
         "220210",
         "20.0",
         "2168-07-10",
         " Respiratory Rate",
         "RespRate",
         "5",
         "100",
         "BPM"
        ],
        [
         "24",
         "12239",
         "266",
         "186251",
         "2168-07-11 17:00:00",
         "220277",
         "100.0",
         "2168-07-10",
         " peripheral",
         "SpO2",
         "5",
         "100",
         "%"
        ],
        [
         "25",
         "13721",
         "266",
         "186251",
         "2168-07-11 00:00:00",
         "220210",
         "21.0",
         "2168-07-10",
         " Respiratory Rate",
         "RespRate",
         "5",
         "100",
         "BPM"
        ],
        [
         "26",
         "13722",
         "266",
         "186251",
         "2168-07-11 00:00:00",
         "220277",
         "100.0",
         "2168-07-10",
         " peripheral",
         "SpO2",
         "5",
         "100",
         "%"
        ],
        [
         "27",
         "13723",
         "266",
         "186251",
         "2168-07-11 00:00:00",
         "223761",
         "37.666666666666664",
         "2168-07-10",
         " Temperature Fahrenheit",
         "TempC",
         "68",
         "113",
         "F"
        ],
        [
         "28",
         "13724",
         "266",
         "186251",
         "2168-07-11 01:00:00",
         "220045",
         "87.0",
         "2168-07-10",
         " Heart Rate",
         "HeartRate",
         "10",
         "300",
         "BPM"
        ],
        [
         "29",
         "13725",
         "266",
         "186251",
         "2168-07-11 01:00:00",
         "220179",
         "125.0",
         "2168-07-10",
         " Non Invasive Blood Pressure systolic",
         "SysBP",
         "20",
         "250",
         "mmHg"
        ],
        [
         "30",
         "13726",
         "266",
         "186251",
         "2168-07-11 01:00:00",
         "220180",
         "66.0",
         "2168-07-10",
         " Non Invasive Blood Pressure diastolic",
         "DiasBP",
         "20",
         "240",
         "mmHg"
        ],
        [
         "31",
         "13727",
         "266",
         "186251",
         "2168-07-11 01:00:00",
         "220181",
         "80.0",
         "2168-07-10",
         " Non Invasive Blood Pressure mean",
         "MeanBP",
         "20",
         "245",
         "mmHg"
        ],
        [
         "32",
         "13728",
         "266",
         "186251",
         "2168-07-11 01:00:00",
         "220210",
         "20.0",
         "2168-07-10",
         " Respiratory Rate",
         "RespRate",
         "5",
         "100",
         "BPM"
        ],
        [
         "33",
         "13729",
         "266",
         "186251",
         "2168-07-11 01:00:00",
         "220277",
         "100.0",
         "2168-07-10",
         " peripheral",
         "SpO2",
         "5",
         "100",
         "%"
        ],
        [
         "34",
         "13730",
         "266",
         "186251",
         "2168-07-11 02:00:00",
         "220045",
         "80.0",
         "2168-07-10",
         " Heart Rate",
         "HeartRate",
         "10",
         "300",
         "BPM"
        ],
        [
         "35",
         "13731",
         "266",
         "186251",
         "2168-07-11 02:00:00",
         "220179",
         "106.0",
         "2168-07-10",
         " Non Invasive Blood Pressure systolic",
         "SysBP",
         "20",
         "250",
         "mmHg"
        ],
        [
         "36",
         "13732",
         "266",
         "186251",
         "2168-07-11 02:00:00",
         "220180",
         "53.0",
         "2168-07-10",
         " Non Invasive Blood Pressure diastolic",
         "DiasBP",
         "20",
         "240",
         "mmHg"
        ],
        [
         "37",
         "13733",
         "266",
         "186251",
         "2168-07-11 02:00:00",
         "220181",
         "65.0",
         "2168-07-10",
         " Non Invasive Blood Pressure mean",
         "MeanBP",
         "20",
         "245",
         "mmHg"
        ],
        [
         "38",
         "13734",
         "266",
         "186251",
         "2168-07-11 02:00:00",
         "220210",
         "26.0",
         "2168-07-10",
         " Respiratory Rate",
         "RespRate",
         "5",
         "100",
         "BPM"
        ],
        [
         "39",
         "13735",
         "266",
         "186251",
         "2168-07-11 02:00:00",
         "220277",
         "100.0",
         "2168-07-10",
         " peripheral",
         "SpO2",
         "5",
         "100",
         "%"
        ],
        [
         "40",
         "13736",
         "266",
         "186251",
         "2168-07-11 02:00:00",
         "225664",
         "147.0",
         "2168-07-10",
         " Glucose finger stick",
         "Glucose",
         "15",
         "2000",
         "mg/dL"
        ],
        [
         "41",
         "13737",
         "266",
         "186251",
         "2168-07-11 02:14:00",
         "220621",
         "138.0",
         "2168-07-10",
         " Glucose (serum)",
         "Glucose",
         "15",
         "2000",
         "mg/dL"
        ],
        [
         "42",
         "13738",
         "266",
         "186251",
         "2168-07-11 03:00:00",
         "220045",
         "85.0",
         "2168-07-10",
         " Heart Rate",
         "HeartRate",
         "10",
         "300",
         "BPM"
        ],
        [
         "43",
         "13739",
         "266",
         "186251",
         "2168-07-11 03:00:00",
         "220179",
         "122.0",
         "2168-07-10",
         " Non Invasive Blood Pressure systolic",
         "SysBP",
         "20",
         "250",
         "mmHg"
        ],
        [
         "44",
         "13740",
         "266",
         "186251",
         "2168-07-11 03:00:00",
         "220180",
         "68.0",
         "2168-07-10",
         " Non Invasive Blood Pressure diastolic",
         "DiasBP",
         "20",
         "240",
         "mmHg"
        ],
        [
         "45",
         "13741",
         "266",
         "186251",
         "2168-07-11 03:00:00",
         "220181",
         "81.0",
         "2168-07-10",
         " Non Invasive Blood Pressure mean",
         "MeanBP",
         "20",
         "245",
         "mmHg"
        ],
        [
         "46",
         "13742",
         "266",
         "186251",
         "2168-07-11 03:00:00",
         "220210",
         "18.0",
         "2168-07-10",
         " Respiratory Rate",
         "RespRate",
         "5",
         "100",
         "BPM"
        ],
        [
         "47",
         "13743",
         "266",
         "186251",
         "2168-07-11 03:00:00",
         "220277",
         "100.0",
         "2168-07-10",
         " peripheral",
         "SpO2",
         "5",
         "100",
         "%"
        ],
        [
         "48",
         "13744",
         "266",
         "186251",
         "2168-07-11 04:00:00",
         "220045",
         "86.0",
         "2168-07-10",
         " Heart Rate",
         "HeartRate",
         "10",
         "300",
         "BPM"
        ],
        [
         "49",
         "13745",
         "266",
         "186251",
         "2168-07-11 04:00:00",
         "220179",
         "120.0",
         "2168-07-10",
         " Non Invasive Blood Pressure systolic",
         "SysBP",
         "20",
         "250",
         "mmHg"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 7043924
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>itemid</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>admittime</th>\n",
       "      <th>measurement name</th>\n",
       "      <th>feature name</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12215</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220179</td>\n",
       "      <td>132.0</td>\n",
       "      <td>2168-07-10</td>\n",
       "      <td>Non Invasive Blood Pressure systolic</td>\n",
       "      <td>SysBP</td>\n",
       "      <td>20</td>\n",
       "      <td>250</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12216</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220180</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2168-07-10</td>\n",
       "      <td>Non Invasive Blood Pressure diastolic</td>\n",
       "      <td>DiasBP</td>\n",
       "      <td>20</td>\n",
       "      <td>240</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12217</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220181</td>\n",
       "      <td>89.0</td>\n",
       "      <td>2168-07-10</td>\n",
       "      <td>Non Invasive Blood Pressure mean</td>\n",
       "      <td>MeanBP</td>\n",
       "      <td>20</td>\n",
       "      <td>245</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12218</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220210</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2168-07-10</td>\n",
       "      <td>Respiratory Rate</td>\n",
       "      <td>RespRate</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>BPM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12219</td>\n",
       "      <td>266</td>\n",
       "      <td>186251</td>\n",
       "      <td>2168-07-11 14:00:00</td>\n",
       "      <td>220277</td>\n",
       "      <td>99.0</td>\n",
       "      <td>2168-07-10</td>\n",
       "      <td>peripheral</td>\n",
       "      <td>SpO2</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069129</th>\n",
       "      <td>14088281</td>\n",
       "      <td>32302</td>\n",
       "      <td>193469</td>\n",
       "      <td>2165-10-13 06:00:00</td>\n",
       "      <td>8441</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2165-10-13</td>\n",
       "      <td>NBP [Diastolic]</td>\n",
       "      <td>DiasBP</td>\n",
       "      <td>20</td>\n",
       "      <td>240</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069130</th>\n",
       "      <td>14088282</td>\n",
       "      <td>32302</td>\n",
       "      <td>193469</td>\n",
       "      <td>2165-10-13 06:15:00</td>\n",
       "      <td>8441</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2165-10-13</td>\n",
       "      <td>NBP [Diastolic]</td>\n",
       "      <td>DiasBP</td>\n",
       "      <td>20</td>\n",
       "      <td>240</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069131</th>\n",
       "      <td>14088283</td>\n",
       "      <td>32302</td>\n",
       "      <td>193469</td>\n",
       "      <td>2165-10-13 06:30:00</td>\n",
       "      <td>8368</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2165-10-13</td>\n",
       "      <td>Arterial BP [Diastolic]</td>\n",
       "      <td>DiasBP</td>\n",
       "      <td>20</td>\n",
       "      <td>240</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069132</th>\n",
       "      <td>14088284</td>\n",
       "      <td>32302</td>\n",
       "      <td>193469</td>\n",
       "      <td>2165-10-13 06:30:00</td>\n",
       "      <td>8441</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2165-10-13</td>\n",
       "      <td>NBP [Diastolic]</td>\n",
       "      <td>DiasBP</td>\n",
       "      <td>20</td>\n",
       "      <td>240</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069133</th>\n",
       "      <td>14088285</td>\n",
       "      <td>32303</td>\n",
       "      <td>103010</td>\n",
       "      <td>2168-09-08 03:00:00</td>\n",
       "      <td>8368</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2168-09-06</td>\n",
       "      <td>Arterial BP [Diastolic]</td>\n",
       "      <td>DiasBP</td>\n",
       "      <td>20</td>\n",
       "      <td>240</td>\n",
       "      <td>mmHg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7043924 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  subject_id  hadm_id            charttime  itemid  \\\n",
       "0             12215         266   186251  2168-07-11 14:00:00  220179   \n",
       "1             12216         266   186251  2168-07-11 14:00:00  220180   \n",
       "2             12217         266   186251  2168-07-11 14:00:00  220181   \n",
       "3             12218         266   186251  2168-07-11 14:00:00  220210   \n",
       "4             12219         266   186251  2168-07-11 14:00:00  220277   \n",
       "...             ...         ...      ...                  ...     ...   \n",
       "7069129    14088281       32302   193469  2165-10-13 06:00:00    8441   \n",
       "7069130    14088282       32302   193469  2165-10-13 06:15:00    8441   \n",
       "7069131    14088283       32302   193469  2165-10-13 06:30:00    8368   \n",
       "7069132    14088284       32302   193469  2165-10-13 06:30:00    8441   \n",
       "7069133    14088285       32303   103010  2168-09-08 03:00:00    8368   \n",
       "\n",
       "         valuenum   admittime                        measurement name  \\\n",
       "0           132.0  2168-07-10    Non Invasive Blood Pressure systolic   \n",
       "1            78.0  2168-07-10   Non Invasive Blood Pressure diastolic   \n",
       "2            89.0  2168-07-10        Non Invasive Blood Pressure mean   \n",
       "3            17.0  2168-07-10                        Respiratory Rate   \n",
       "4            99.0  2168-07-10                              peripheral   \n",
       "...           ...         ...                                     ...   \n",
       "7069129      86.0  2165-10-13                         NBP [Diastolic]   \n",
       "7069130      68.0  2165-10-13                         NBP [Diastolic]   \n",
       "7069131      53.0  2165-10-13                 Arterial BP [Diastolic]   \n",
       "7069132      56.0  2165-10-13                         NBP [Diastolic]   \n",
       "7069133      80.0  2168-09-06                 Arterial BP [Diastolic]   \n",
       "\n",
       "        feature name  min  max units  \n",
       "0              SysBP   20  250  mmHg  \n",
       "1             DiasBP   20  240  mmHg  \n",
       "2             MeanBP   20  245  mmHg  \n",
       "3           RespRate    5  100   BPM  \n",
       "4               SpO2    5  100     %  \n",
       "...              ...  ...  ...   ...  \n",
       "7069129       DiasBP   20  240  mmHg  \n",
       "7069130       DiasBP   20  240  mmHg  \n",
       "7069131       DiasBP   20  240  mmHg  \n",
       "7069132       DiasBP   20  240  mmHg  \n",
       "7069133       DiasBP   20  240  mmHg  \n",
       "\n",
       "[7043924 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.sort_values(['subject_id', 'hadm_id', 'charttime'])\n",
    "labs_features_names = set(labs['feature name'])\n",
    "vits_features_names = set(vits['feature name'])\n",
    "labs_features = [col for col in merged.columns if col.split('_')[0] in labs_features_names]\n",
    "vits_features = [col for col in merged.columns if col.split('_')[0] in vits_features_names]\n",
    "\n",
    "lab_diff_cols = {}\n",
    "for col in labs_features:\n",
    "    if col.find(\"mean\") >= 0:\n",
    "      base = merged.groupby(['subject_id', 'hadm_id'])[col].transform('first')\n",
    "      lab_diff_cols[f'{col}_diff'] = merged[col] - base\n",
    "\n",
    "lab_diff_df = pd.DataFrame(lab_diff_cols)\n",
    "\n",
    "vital_diff_cols = {}\n",
    "for col in vits_features:\n",
    "  if col.find(\"mean\") >= 0:\n",
    "      diff_series = merged.groupby(['subject_id', 'hadm_id'])[col].diff()\n",
    "      vital_diff_cols[f'{col}_diff'] = diff_series\n",
    "\n",
    "vital_diff_df = pd.DataFrame(vital_diff_cols)\n",
    "\n",
    "# Concatenate back to original DataFrame\n",
    "merged = pd.concat([merged, lab_diff_df, vital_diff_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['charttime'] = pd.to_datetime(merged['charttime'], errors='coerce')\n",
    "\n",
    "time_to_death = (merged['dod'] - merged['charttime']).dt.total_seconds() / (60 * 60)\n",
    "merged['target'] = (pd.notnull(time_to_death) & (time_to_death <= pred_window + pred_gap)).astype(int)\n",
    "merged = merged[time_to_death.isna() | (time_to_death >= pred_gap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_max_chartime = merged.groupby(\"subject_id\").agg({\"charttime\": [\"min\", \"max\"]})\n",
    "#min_max_chartime.columns = ['min_charttime', 'max_charttime']\n",
    "#min_max_chartime.to_csv('data/min_max_chartime.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "merged_clean = merged.reset_index(drop=True)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "#Split to train & test (all data of a single patient needs to be in the same group)\n",
    "X = merged_clean\n",
    "y = merged_clean['target']\n",
    "groups = merged_clean['subject_id']\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, test_size=0.1)\n",
    "train_index, test_index = next(gss.split(X, y, groups))\n",
    "val_index = list(set(X.index.to_list()) - (set(train_index.tolist()) | set(test_index.tolist())))\n",
    "\n",
    "X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "X_val, y_val = X.iloc[val_index], y.iloc[val_index]\n",
    "X_test, y_test = X.iloc[test_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 276696, 276697, 276698])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pooled embeddings shape: 24796\n",
      "Number of subjects: 24796\n"
     ]
    }
   ],
   "source": [
    "import sys, numpy\n",
    "import pickle\n",
    "# Create an alias so pickle can find the old path\n",
    "sys.modules[\"numpy._core.numeric\"] = numpy.core.numeric  \n",
    "\n",
    "with open(\"data/notes_with_embeddings.pkl\", \"rb\") as f:\n",
    "    notes = pickle.load(f)\n",
    "    notes_ordered = X[['subject_id']].drop_duplicates().merge(\n",
    "    notes[[\"subject_id\",\"embeddings\"]], \n",
    "    on='subject_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "    embeddings_dict = {}\n",
    "\n",
    "for idx, row in notes_ordered.iterrows():\n",
    "    subject_id = row['subject_id']\n",
    "    embeddings = row['embeddings']\n",
    "    try:\n",
    "        if isinstance(embeddings, np.ndarray) :\n",
    "            # Convert to tensor if it's not already\n",
    "            if not isinstance(embeddings, torch.Tensor):\n",
    "                embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "            \n",
    "            # Perform average pooling across the sequence dimension\n",
    "            # Assuming embeddings shape is (sequence_length, embedding_dim)\n",
    "            pooled_embedding = torch.mean(embeddings, dim=0)  # Shape: (embedding_dim,)\n",
    "            embeddings_dict[subject_id] = pooled_embedding\n",
    "        else:\n",
    "            # Handle missing embeddings with zero vector\n",
    "            # Assuming embedding dimension is 768 (common for transformers)\n",
    "            embeddings_dict[subject_id] = torch.zeros(768, dtype=torch.float32)\n",
    "    except Exception as e:\n",
    "        flag = 1\n",
    "\n",
    "# Convert to a tensor where each row corresponds to a subject_id\n",
    "subject_ids_list = notes_ordered['subject_id'].tolist()\n",
    "pooled_embeddings = [embeddings_dict[subject_id] for subject_id in subject_ids_list]\n",
    "\n",
    "print(f\"Pooled embeddings shape: {len(pooled_embeddings)}\")\n",
    "print(f\"Number of subjects: {len(subject_ids_list)}\")\n",
    "\n",
    "notes_df = pd.DataFrame({\n",
    "    'subject_id': subject_ids_list,\n",
    "    'embeddings': pooled_embeddings}).set_index('subject_id')\n",
    "\n",
    "notes_df_train = notes_df.loc[X_train.subject_id.unique()]\n",
    "notes_df_val = notes_df.loc[X_val.subject_id.unique()]\n",
    "notes_df_test = notes_df.loc[X_test.subject_id.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "4917226d-4753-47db-af62-fff2401449cb",
       "rows": [
        [
         "0",
         "0"
        ],
        [
         "1",
         "0"
        ],
        [
         "2",
         "0"
        ],
        [
         "3",
         "0"
        ],
        [
         "4",
         "0"
        ],
        [
         "5",
         "0"
        ],
        [
         "6",
         "0"
        ],
        [
         "7",
         "0"
        ],
        [
         "8",
         "0"
        ],
        [
         "9",
         "0"
        ],
        [
         "10",
         "0"
        ],
        [
         "11",
         "0"
        ],
        [
         "12",
         "0"
        ],
        [
         "13",
         "0"
        ],
        [
         "14",
         "0"
        ],
        [
         "15",
         "0"
        ],
        [
         "16",
         "0"
        ],
        [
         "17",
         "0"
        ],
        [
         "18",
         "0"
        ],
        [
         "19",
         "0"
        ],
        [
         "20",
         "0"
        ],
        [
         "21",
         "0"
        ],
        [
         "22",
         "0"
        ],
        [
         "23",
         "0"
        ],
        [
         "24",
         "0"
        ],
        [
         "25",
         "1"
        ],
        [
         "26",
         "1"
        ],
        [
         "27",
         "1"
        ],
        [
         "28",
         "1"
        ],
        [
         "29",
         "1"
        ],
        [
         "30",
         "1"
        ],
        [
         "31",
         "1"
        ],
        [
         "32",
         "1"
        ],
        [
         "33",
         "1"
        ],
        [
         "34",
         "1"
        ],
        [
         "35",
         "1"
        ],
        [
         "36",
         "1"
        ],
        [
         "37",
         "1"
        ],
        [
         "38",
         "1"
        ],
        [
         "39",
         "1"
        ],
        [
         "40",
         "1"
        ],
        [
         "41",
         "0"
        ],
        [
         "42",
         "0"
        ],
        [
         "43",
         "0"
        ],
        [
         "44",
         "0"
        ],
        [
         "45",
         "0"
        ],
        [
         "46",
         "0"
        ],
        [
         "47",
         "0"
        ],
        [
         "48",
         "0"
        ],
        [
         "49",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 221658
       }
      },
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "276694    0\n",
       "276695    0\n",
       "276696    0\n",
       "276697    0\n",
       "276698    0\n",
       "Name: target, Length: 221658, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0083, -0.0111, -0.0092,  ...,  0.0204,  0.0005,  0.0055],\n",
       "        [ 0.0021, -0.0124, -0.0153,  ...,  0.0157,  0.0012,  0.0019],\n",
       "        [ 0.0088, -0.0140, -0.0083,  ...,  0.0220, -0.0032,  0.0039],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0091, -0.0106, -0.0140,  ...,  0.0099,  0.0144,  0.0093],\n",
       "        [ 0.0056, -0.0131, -0.0198,  ...,  0.0136,  0.0302,  0.0245]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19736])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Test Results - Accuracy: 0.9254, AUC: 0.6484, AP: 0.1032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, average_precision_score\n",
    "\n",
    "import torch.nn as nn\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=128, dropout=0.1):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x).squeeze(-1)\n",
    "\n",
    "# Get embeddings and labels for train/val/test\n",
    "train_embeddings = torch.stack(notes_df_train.embeddings.values.tolist()).to(DEVICE)\n",
    "val_embeddings = torch.stack(notes_df_val.embeddings.values.tolist()).to(DEVICE)\n",
    "test_embeddings = torch.stack(notes_df_test.embeddings.values.tolist()).to(DEVICE)\n",
    "\n",
    "# Get labels by grouping and taking max (if any timestep has target=1, patient has target=1)\n",
    "train_labels = X_train.groupby('subject_id')['target'].max().reindex(notes_df_train.index).values\n",
    "val_labels = X_val.groupby('subject_id')['target'].max().reindex(notes_df_val.index).values\n",
    "test_labels = X_test.groupby('subject_id')['target'].max().reindex(notes_df_test.index).values\n",
    "\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32).to(DEVICE)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.float32).to(DEVICE)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Create data loaders for mini-batch training\n",
    "train_dataset = torch.utils.data.TensorDataset(train_embeddings, train_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(val_embeddings, val_labels)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_embeddings, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Calculate pos_weight for weighted BCE loss\n",
    "pos_weight = (train_labels == 0).sum().float() / (train_labels == 1).sum().float()\n",
    "# Initialize model\n",
    "mlp_model = SimpleMLP().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training\n",
    "# Training loop with validation\n",
    "for epoch in range(1):\n",
    "    mlp_model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_embeddings, batch_labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(batch_embeddings)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    mlp_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_embeddings, batch_labels in val_loader:\n",
    "            outputs = mlp_model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "# Evaluation\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = mlp_model(test_embeddings)\n",
    "    test_probs = torch.sigmoid(test_outputs).cpu().numpy()\n",
    "    test_preds = (test_probs > 0.5).astype(int)\n",
    "    test_labels_np = test_labels.cpu().numpy()\n",
    "    \n",
    "    acc = accuracy_score(test_labels_np, test_preds)\n",
    "    auc = roc_auc_score(test_labels_np, test_probs)\n",
    "    ap = average_precision_score(test_labels_np, test_probs)\n",
    "    \n",
    "    print(f'MLP Test Results - Accuracy: {acc:.4f}, AUC: {auc:.4f}, AP: {ap:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df = pd.DataFrame({\n",
    "    'subject_id': subject_ids_list,\n",
    "    'embeddings': pooled_embeddings}).set_index('subject_id')\n",
    "\n",
    "notes_df_train = notes_df.loc[X_train.subject_id.unique()]\n",
    "notes_df_val = notes_df.loc[X_val.subject_id.unique()]\n",
    "notes_df_test = notes_df.loc[X_test.subject_id.unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_cols = X_train.select_dtypes(include='float').columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train.loc[:, num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_val.loc[:, num_cols] = scaler.transform(X_val[num_cols])\n",
    "X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "baseline_df = X_train[X_train.charttime.dt.date == X_train.admittime.dt.date].mean(axis=0).fillna(0)\n",
    "X_train.loc[:, num_cols] = X_train[num_cols].fillna(baseline_df)\n",
    "X_val.loc[:, num_cols] = X_val[num_cols].fillna(baseline_df)\n",
    "X_test.loc[:, num_cols] = X_test[num_cols].fillna(baseline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['hadm_id','icustay_id','intime','outtime','admittime', 'dischtime', 'dod','dob', 'mort', 'los_hosp_hr', 'charttime','adm_to_death']\n",
    "\n",
    "to_keep = ~X_train.columns.isin(to_drop)\n",
    "to_keep = X_train.columns[to_keep]\n",
    "X_train = X_train[to_keep]\n",
    "X_test = X_test[to_keep]\n",
    "X_val = X_val[to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "def cluster_and_select_subjects(X_train, num_clusters=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate the first row of each subject_id in X_train, cluster it to num_clusters \n",
    "    and choose one subject_id from each cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: DataFrame with subject_id column\n",
    "    num_clusters: int, number of clusters to create\n",
    "    random_state: int, for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    list: selected subject_ids, one from each cluster\n",
    "    \"\"\"\n",
    "    first_rows = X_train.groupby('subject_id').first().reset_index()\n",
    "    \n",
    "    features_for_clustering = first_rows.drop('subject_id', axis=1)\n",
    "    \n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=random_state, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(features_for_clustering)\n",
    "    \n",
    "\n",
    "    first_rows['cluster'] = cluster_labels\n",
    "    \n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    selected_subjects = []\n",
    "    \n",
    "    for cluster_id in range(num_clusters):\n",
    "        cluster_subjects = first_rows[first_rows['cluster'] == cluster_id]['subject_id'].values\n",
    "        if len(cluster_subjects) > 0:\n",
    "            selected_subject = np.random.choice(cluster_subjects)\n",
    "            selected_subjects.append(selected_subject)\n",
    "    \n",
    "    return selected_subjects\n",
    "\n",
    "selected_subjects = cluster_and_select_subjects(X_train, num_clusters=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X_core with rows where subject_id is in selected_subjects\n",
    "X_core = X_train[X_train['subject_id'].isin(selected_subjects)]\n",
    "\n",
    "# Update X_train to exclude the selected subjects\n",
    "X_train = X_train[~X_train['subject_id'].isin(selected_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_series_data(df, group_col=\"subject_id\", maxlen=18):\n",
    "  grouped = df.groupby(group_col)\n",
    "  subject_sequences = [group.values[:, 1:] for _, group in grouped]\n",
    "  padded_tensor = pad_sequences(subject_sequences, padding='post', dtype='float32')\n",
    "  sequence_lengths = [len(seq) for seq in subject_sequences]\n",
    "  padding_mask = np.zeros((len(sequence_lengths), maxlen), dtype=np.float32)\n",
    "  for i, length in enumerate(sequence_lengths):\n",
    "      padding_mask[i, :length] = 1.0\n",
    "  labels = padded_tensor[:,:,-1]\n",
    "  padded_tensor = padded_tensor[:,:,:-1]\n",
    "  padded_tensor = torch.tensor(padded_tensor, dtype=torch.float32)\n",
    "  labels = torch.tensor(labels, dtype=torch.float32)\n",
    "  padding_mask = torch.tensor(padding_mask, dtype=torch.float32)\n",
    "  return padded_tensor, labels, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import NoteEmbedder\n",
    "#import imp\n",
    "#os.environ[\"TRANSFORMERS_CACHE\"] = \"/home/bnet/ronsheinin/MLHC/MLHC/cache\"\n",
    "#imp.reload(NoteEmbedder)\n",
    "#import os\n",
    "#notes = pd.read_csv('data/notes.csv')\n",
    "#note_embeddings = NoteEmbedder.run_embeeding(notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, X, y, padding_mask, notes,  k=5, single_patient=False):\n",
    "        # self.core = core\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.padding_mask = padding_mask\n",
    "        # self.padding_mask_core = padding_mask_core\n",
    "        self.k = k\n",
    "        self.notes = notes\n",
    "        self.single_patient = single_patient\n",
    "        if self.single_patient:\n",
    "            self.cal_graphs()\n",
    "\n",
    "        self.edge_list = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.padding_mask[idx], self.notes[idx]\n",
    "    def cal_graphs(self):\n",
    "        if self.edge_list is not None:\n",
    "            return self.edge_list\n",
    "        \n",
    "        edge_list = []\n",
    "        for patient_idx in range(len(self.X)):\n",
    "            edges = self.build_knn_graph(self.X[patient_idx:patient_idx+1], self.core, \n",
    "                                                self.padding_mask[patient_idx:patient_idx+1], self.padding_mask_core, k=self.k)\n",
    "            edge_list.append(edges)\n",
    "        self.edge_list = torch.cat(edge_list, dim=1).T\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_knn_graph(batch, core, padding_mask_batch, padding_mask_core, k=5):\n",
    "        \"\"\"\n",
    "        Build a KNN graph from batch and core tensors.\n",
    "        \n",
    "        Args:\n",
    "            batch: 3D tensor (batch_size, seq_len, features)\n",
    "            core: 3D tensor (core_size, seq_len, features)\n",
    "            padding_mask_batch: 2D tensor (batch_size, seq_len) indicating valid time points\n",
    "            padding_mask_core: 2D tensor (core_size, seq_len) indicating valid time points\n",
    "            k: number of nearest neighbors for patient connections\n",
    "        \n",
    "        Returns:\n",
    "            edge_index: tensor of shape (2, num_edges) representing graph edges\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = batch.shape\n",
    "        core_size = core.shape[0]\n",
    "        total_patients = batch_size + core_size\n",
    "        batch_size = batch.shape[0]\n",
    "\n",
    "        all_patients = torch.cat([batch, core], dim=0)\n",
    "        all_padding_mask = torch.cat([padding_mask_batch, padding_mask_core], dim=0)\n",
    "        \n",
    "        edges = []\n",
    "        \n",
    "        for patient_idx in range(total_patients):\n",
    "            for t in range(seq_len - 1):\n",
    "            \n",
    "                if all_padding_mask[patient_idx, t] > 0 and all_padding_mask[patient_idx, t + 1] > 0:\n",
    "                    node_curr = patient_idx * seq_len + t\n",
    "                    node_next = patient_idx * seq_len + t + 1\n",
    "   \n",
    "                    edges.append([node_curr, node_next])\n",
    "                    edges.append([node_next, node_curr])\n",
    "        \n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            valid_patients = all_padding_mask[:, t] > 0\n",
    "            valid_indices = torch.where(valid_patients)[0] \n",
    "            \n",
    "\n",
    "            valid_patients_to = all_padding_mask[batch_size:, t] > 0\n",
    "            valid_indices_to = torch.where(valid_patients_to)[0] + batch_size\n",
    "\n",
    "\n",
    "            if len(valid_indices) > 1:\n",
    "                features_t = all_patients[valid_indices, t, :]\n",
    "                features_t_to = all_patients[valid_indices_to, t, :]\n",
    "\n",
    "                distances = torch.cdist(features_t, features_t_to, p=2)\n",
    "                \n",
    "                for i, patient_idx in enumerate(valid_indices):\n",
    "                    num_neighbors = min(k, len(valid_indices_to))\n",
    "                    _, nearest_indices = torch.topk(distances[i], num_neighbors, largest=False)\n",
    "                    \n",
    "                    for j in nearest_indices:\n",
    "                        neighbor_idx = valid_indices_to[j]\n",
    "                        node_curr = patient_idx * seq_len + t\n",
    "                        node_neighbor = neighbor_idx * seq_len + t\n",
    "                        edges.append([node_neighbor, node_curr])\n",
    "        \n",
    "        if edges:\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        \n",
    "        return edge_index\n",
    "\n",
    "#dataset = PatientDataset(core=padd_tensor_core, X=padded_tensor, y=labels, padding_mask=padding_mask, padding_mask_core=padding_mask_core, k=5)\n",
    "# dataset.cal_graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#DEVICE = torch.device('cpu')  # For testing purposes, use CPU\n",
    "\n",
    "\n",
    "class GraphGRUMortalityModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n1_gat_layers, n2_gru_layers, X_core, core_padding_mask,\n",
    "                 num_heads=4, dropout=0.1, seq_len=18, k=5, gnn_flag=True):\n",
    "        \"\"\"\n",
    "        Mortality prediction model with Graph Attention + GRU layers\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Input feature dimension\n",
    "            hidden_dim: Hidden dimension for GAT and GRU layers\n",
    "            n1_gat_layers: Number of Graph Attention layers\n",
    "            n2_gru_layers: Number of GRU layers\n",
    "            X_core_dim: Core set dimension (number of core patients)\n",
    "            num_heads: Number of attention heads for GAT\n",
    "            dropout: Dropout rate\n",
    "            seq_len: Sequence length\n",
    "        \"\"\"\n",
    "        super(GraphGRUMortalityModel, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n1_gat_layers = n1_gat_layers\n",
    "        self.n2_gru_layers = n2_gru_layers\n",
    "        self.X_core = X_core.to(DEVICE)\n",
    "        self.core_padding_mask = core_padding_mask.to(DEVICE)\n",
    "        self.seq_len = seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.gnn_flag = gnn_flag\n",
    "\n",
    "\n",
    "        self.notes_layer = nn.Linear(768, hidden_dim).to(DEVICE) \n",
    "        self.gat_layers = nn.ModuleList().to(DEVICE)\n",
    "        \n",
    "        self.gat_layers.append(\n",
    "            GATv2Conv(input_dim, hidden_dim // num_heads, heads=num_heads, dropout=dropout, concat=True)\n",
    "        )\n",
    "        \n",
    "        for _ in range(n1_gat_layers - 1):\n",
    "            self.gat_layers.append(\n",
    "                GATv2Conv(hidden_dim, hidden_dim // num_heads, heads=num_heads, dropout=dropout, concat=True)\n",
    "            )\n",
    "        \n",
    "        if self.gnn_flag:\n",
    "            self.gru = nn.GRU(hidden_dim, hidden_dim, n2_gru_layers, batch_first=True, dropout=dropout)\n",
    "        else:\n",
    "            self.gru = nn.GRU(input_dim, hidden_dim, n2_gru_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(4*hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout).to(DEVICE)\n",
    "        self.k = k\n",
    "\n",
    "        self.best_model = None\n",
    "\n",
    "        # initialize weights\n",
    "        for layer in self.gat_layers:\n",
    "            if isinstance(layer, GATv2Conv):\n",
    "                nn.init.xavier_uniform_(layer.lin_l.weight)\n",
    "                nn.init.xavier_uniform_(layer.lin_r.weight) \n",
    "        nn.init.xavier_uniform_(self.gru.weight_ih_l0)\n",
    "        nn.init.xavier_uniform_(self.gru.weight_hh_l0)\n",
    "        nn.init.constant_(self.gru.bias_ih_l0, 0.0)\n",
    "        nn.init.constant_(self.gru.bias_hh_l0, 0.0)\n",
    "        \n",
    "  \n",
    "        \n",
    "    def forward(self, x ,padding_mask, edge_index, nots):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            core: Core patients tensor (X_core_dim, seq_len, input_dim)\n",
    "            x: Batch patients tensor (batch_size, seq_len, input_dim)\n",
    "            y: Target labels (batch_size, seq_len)\n",
    "            padding_mask: Padding mask (batch_size, seq_len)\n",
    "            edge_index: Graph edge indices (2, num_edges)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: Mortality predictions (batch_size, seq_len, 1)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        if self.gnn_flag:\n",
    "            all_patients = torch.cat([x, self.X_core], dim=0)  # (batch_size + X_core_dim, seq_len, input_dim)\n",
    "            total_patients = batch_size + self.X_core.shape[0]\n",
    "            \n",
    "            # Reshape for graph processing: (total_patients * seq_len, input_dim)\n",
    "            graph_input = all_patients.view(total_patients * self.seq_len, -1)\n",
    "            # Apply GAT layers\n",
    "            for gat_layer in self.gat_layers:\n",
    "                graph_input = F.relu(gat_layer(graph_input, edge_index))\n",
    "                # graph_input = self.dropout(graph_input)\n",
    "            \n",
    "            # Reshape back to sequence format: (total_patients, seq_len, hidden_dim)\n",
    "            graph_output = graph_input.view(total_patients, self.seq_len, -1)\n",
    "\n",
    "            # Extract only batch patients (exclude core)\n",
    "        \n",
    "            batch_output = graph_output[:batch_size]  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        else:\n",
    "            batch_output = x\n",
    "        # Apply GRU layers\n",
    "        # Pack sequences for efficient processing\n",
    "        # lengths from mask (True = pad) → count valid steps\n",
    "        lengths = (padding_mask.to(bool)).sum(dim=1)                       # (batch,)\n",
    "        lengths = lengths.clamp(min=1).cpu()\n",
    "        #packed_input = pack_padded_sequence(batch_output, lengths, batch_first=True, enforce_sorted=False)\n",
    "        #gru_output, _ = self.gru(packed_input)\n",
    "        # Unpack sequences\n",
    "        #gru_output, _ = pad_packed_sequence(gru_output, batch_first=True, total_length=self.seq_len)\n",
    "\n",
    "        mask_index = padding_mask.sum(dim=1).long() - 1  # Get the last valid index for each sequence\n",
    "        mask_expanded = padding_mask.unsqueeze(-1)    \n",
    "        gru_output, _ = self.gru(batch_output)\n",
    "        out = torch.cat([\n",
    "            gru_output[torch.arange(gru_output.size(0)), mask_index, :],\n",
    "            (gru_output * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1),\n",
    "            (gru_output * mask_expanded).max(dim=1)[0]\n",
    "        ], dim=-1) \n",
    "        nots = F.relu(self.notes_layer(nots))\n",
    "        X_concat = torch.cat([out, nots], dim=-1)  # (batch_size, seq_len, 2*hidden_dim)\n",
    "        predictions = self.classifier(X_concat)  # (batch_size, seq_len, 1)\n",
    "\n",
    "        return predictions.squeeze(-1)  # (batch_size, seq_len)\n",
    "    \n",
    "    \n",
    "    def masked_bce_loss(self, logits, targets, mask, pos_weight=None):\n",
    "        T = min(logits.shape[1], targets.shape[1], mask.shape[1])\n",
    "        logits, targets, mask = logits[:, :T], targets[:, :T], mask[:, :T]\n",
    "        loss = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)(logits, targets)\n",
    "        return (loss * mask).sum() / mask.sum()\n",
    "    \n",
    "\n",
    "    def train_all(self, dataloaders, datasets, epochs: int = 10, learning_rate: float = 1e-3, pos_lambda : float = 1):\n",
    "        self.train()\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        best_validation_accuracy = - float('inf')\n",
    "        targets = datasets['train'].y.max(dim=1)[0]  # Get the max target for each patien\n",
    "        pos_weight = (targets == 0).sum() / (targets == 1).sum()  # Adjust pos_weight as needed\n",
    "        print(f'Pos weight: {pos_weight:.4f}')\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f'Starting epoch {epoch + 1}/{epochs}')\n",
    "            total = 0\n",
    "            for x, y, padding_mask, notes in tqdm(dataloaders['train']):\n",
    "                optim.zero_grad()\n",
    "                x, padding_mask, y, notes = x.to(DEVICE), padding_mask.to(DEVICE), y.to(DEVICE), notes.to(DEVICE)\n",
    "                edge_index = datasets['train'].build_knn_graph(x, self.X_core, padding_mask, self.core_padding_mask, k=self.k).to(DEVICE)\n",
    "                predictions = self.forward(x, padding_mask, edge_index, notes)\n",
    "                loss = nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)(predictions, y.max(dim=1)[0])\n",
    "                #loss = F.binary_cross_entropy_with_logits(predictions.view(-1), y.view(-1))\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                \n",
    "                total += loss.item()\n",
    "            avg_loss = total / len(dataloaders['train'])\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "            validation_accuracy = self.validate(dataloaders['val'], datasets['val'])\n",
    "            print(f'Validation Accuracy: {validation_accuracy[0]:.4f} | AUC: {validation_accuracy[1]:.4f} | AP: {validation_accuracy[2]:.4f}')\n",
    "            if validation_accuracy[2] > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy[2]\n",
    "                self.best_model = self.state_dict()\n",
    "                print(\"Best model updated\")\n",
    "        self.load_state_dict(self.best_model)\n",
    "\n",
    "    def validate(self, dataloader, dataset):\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            all_true_labels = []\n",
    "            all_predicted_labels = []\n",
    "            for x, y, padding_mask, nots in dataloader:\n",
    "                x, padding_mask, y, nots = x.to(DEVICE), padding_mask.to(DEVICE), y.to(DEVICE), nots.to(DEVICE)\n",
    "                y = y.max(dim=1)[0]\n",
    "                edge_index = dataset.build_knn_graph(x, self.X_core, padding_mask, self.core_padding_mask, k=self.k).to(DEVICE)\n",
    "                predictions = self.forward(x, padding_mask, edge_index, nots)\n",
    "                all_true_labels.extend(y.cpu().numpy())\n",
    "                all_predicted_labels.extend(torch.sigmoid(predictions).cpu().numpy().flatten())\n",
    "                predicted_labels = (torch.sigmoid(predictions) > 0.5).float()\n",
    "                correct += (predicted_labels == y).sum().item()\n",
    "                total += y.shape[0]\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        self.train()\n",
    "        return accuracy, roc_auc_score(all_true_labels, all_predicted_labels), average_precision_score(all_true_labels, all_predicted_labels)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19736, 768])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(notes_df_train.embeddings.values.tolist()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "hidden_dim = 64\n",
    "k = 11\n",
    "\n",
    "padded_tensor_train, labels_train, padding_mask_train = generate_series_data(X_train, group_col=\"subject_id\", maxlen=18)\n",
    "padded_tensor_val, labels_val, padding_mask_val = generate_series_data(X_val, group_col=\"subject_id\", maxlen=18)\n",
    "padded_tensor_test, labels_test, padding_mask_test = generate_series_data(X_test, group_col=\"subject_id\", maxlen=18)\n",
    "padd_tensor_core, labels_core, padding_mask_core = generate_series_data(X_core, group_col=\"subject_id\", maxlen=18)\n",
    "\n",
    "datasets = {x: PatientDataset(d, y, m, n) for x, d, y, m, n in\n",
    "        zip(['train', 'val', 'test'], [padded_tensor_train, padded_tensor_val, padded_tensor_test], \n",
    "            [labels_train, labels_val, labels_test], \n",
    "            [padding_mask_train, padding_mask_val, padding_mask_test],\n",
    "            [torch.stack(notes_df_train.embeddings.values.tolist()), \n",
    "             torch.stack(notes_df_val.embeddings.values.tolist()), \n",
    "             torch.stack(notes_df_test.embeddings.values.tolist())])}\n",
    "dataloaders = {x: DataLoader(datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'val', 'test']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [04:25<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.0422\n",
      "Validation Accuracy: 0.8718 | AUC: 0.7847 | AP: 0.2756\n",
      "Best model updated\n",
      "Starting epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [04:27<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.9917\n",
      "Validation Accuracy: 0.8238 | AUC: 0.7734 | AP: 0.2768\n",
      "Best model updated\n",
      "Starting epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [04:28<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.9104\n",
      "Validation Accuracy: 0.7887 | AUC: 0.7683 | AP: 0.2802\n",
      "Best model updated\n",
      "Starting epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [04:26<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.8729\n",
      "Validation Accuracy: 0.7669 | AUC: 0.7661 | AP: 0.2618\n",
      "Starting epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/78 [00:05<06:44,  5.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m GraphGRUMortalityModel(input_dim\u001b[38;5;241m=\u001b[39mpadded_tensor_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, n1_gat_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, n2_gru_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, X_core\u001b[38;5;241m=\u001b[39mpadd_tensor_core, \n\u001b[1;32m      2\u001b[0m                                core_padding_mask\u001b[38;5;241m=\u001b[39mpadding_mask_core, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39mpadded_tensor_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], k\u001b[38;5;241m=\u001b[39mk, gnn_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed. Validating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m acc, auc, pr \u001b[38;5;241m=\u001b[39m  model\u001b[38;5;241m.\u001b[39mvalidate(dataloaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[179], line 162\u001b[0m, in \u001b[0;36mGraphGRUMortalityModel.train_all\u001b[0;34m(self, dataloaders, datasets, epochs, learning_rate, pos_lambda)\u001b[0m\n\u001b[1;32m    160\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    161\u001b[0m x, padding_mask, y, notes \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(DEVICE), padding_mask\u001b[38;5;241m.\u001b[39mto(DEVICE), y\u001b[38;5;241m.\u001b[39mto(DEVICE), notes\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> 162\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_knn_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_core\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    163\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x, padding_mask, edge_index, notes)\n\u001b[1;32m    164\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m, pos_weight\u001b[38;5;241m=\u001b[39mpos_weight)(predictions, y\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[125], line 95\u001b[0m, in \u001b[0;36mPatientDataset.build_knn_graph\u001b[0;34m(batch, core, padding_mask_batch, padding_mask_core, k)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 node_curr \u001b[38;5;241m=\u001b[39m patient_idx \u001b[38;5;241m*\u001b[39m seq_len \u001b[38;5;241m+\u001b[39m t\n\u001b[1;32m     94\u001b[0m                 node_neighbor \u001b[38;5;241m=\u001b[39m neighbor_idx \u001b[38;5;241m*\u001b[39m seq_len \u001b[38;5;241m+\u001b[39m t\n\u001b[0;32m---> 95\u001b[0m                 \u001b[43medges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_neighbor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_curr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edges:\n\u001b[1;32m     98\u001b[0m     edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(edges, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mt()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GraphGRUMortalityModel(input_dim=padded_tensor_train.shape[2], hidden_dim=hidden_dim, n1_gat_layers=2, n2_gru_layers=2, X_core=padd_tensor_core, \n",
    "                               core_padding_mask=padding_mask_core, num_heads=4, dropout=0.1, seq_len=padded_tensor_train.shape[1], k=k, gnn_flag=True).to(DEVICE)\n",
    "\n",
    "model.train_all(dataloaders, datasets, epochs=10, learning_rate=1e-3)\n",
    "print(\"Training completed. Validating on test set...\")\n",
    "\n",
    "acc, auc, pr =  model.validate(dataloaders['test'], datasets['test'])\n",
    "print(f'Test Accuracy in BaseLine model: {acc:.4f}, AUC: {auc:.4f}, AP: {pr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy in BaseLine model: 0.7843, AUC: 0.8381, AP: 0.3395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "acc, auc, pr =  model.validate(dataloaders['test'], datasets['test'])\n",
    "print(f'Test Accuracy in BaseLine model: {acc:.4f}, AUC: {auc:.4f}, AP: {pr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-18 16:49:33,775] A new study created in memory with name: no-name-909f0b03-19ae-42e6-aaf8-4896d952bb34\n",
      "/home/bnet/kupershmidt/anaconda3/envs/spider/lib/python3.10/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos weight: 11.4991\n",
      "Starting epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 25/78 [01:23<02:58,  3.36s/it]"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import gc\n",
    "import joblib\n",
    "def objective_mlp(trial):\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', 32, 128, step=16)\n",
    "    n1_gat_layers = trial.suggest_int('n1_gat_layers', 1, 5)\n",
    "    n2_gru_layers = trial.suggest_int('n2_gru_layers', 1, 5)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5, step=0.1)\n",
    "    num_heads = trial.suggest_int('num_heads', 2, 8, step=2)\n",
    "    k = trial.suggest_int('k', 5, 15, step=2)\n",
    "    lr= trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    model = GraphGRUMortalityModel(input_dim=padded_tensor_train.shape[2], hidden_dim=hidden_dim, n1_gat_layers=n1_gat_layers,\n",
    "                                   n2_gru_layers=n2_gru_layers, X_core=padd_tensor_core, core_padding_mask=padding_mask_core,\n",
    "                                   num_heads=num_heads, dropout=dropout, seq_len=padded_tensor_train.shape[1], k=k, gnn_flag=True).to(DEVICE)\n",
    "\n",
    "    model.train_all(dataloaders, datasets, epochs=10, learning_rate=lr)\n",
    "    \n",
    "    acc, auc, pr = model.validate(dataloaders['test'], datasets['test'])\n",
    "    \n",
    "    del model \n",
    "    torch.cuda.empty_cache() \n",
    "    gc.collect()\n",
    "    return pr\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_mlp, n_trials=20)  \n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params) \n",
    "joblib.dump(study, 'optuna_study.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=1, state=TrialState.COMPLETE, values=[0.3055693844824112], datetime_start=datetime.datetime(2025, 8, 17, 19, 18, 21, 898015), datetime_complete=datetime.datetime(2025, 8, 17, 19, 53, 35, 836441), params={'hidden_dim': 48, 'n1_gat_layers': 3, 'n2_gru_layers': 2, 'dropout': 0.1, 'num_heads': 8, 'k': 5}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'hidden_dim': IntDistribution(high=128, log=False, low=32, step=16), 'n1_gat_layers': IntDistribution(high=5, log=False, low=1, step=1), 'n2_gru_layers': IntDistribution(high=5, log=False, low=1, step=1), 'dropout': FloatDistribution(high=0.5, log=False, low=0.1, step=0.1), 'num_heads': IntDistribution(high=8, log=False, low=2, step=2), 'k': IntDistribution(high=15, log=False, low=5, step=2)}, trial_id=1, value=None)\n",
      "Best hyperparameters: {'hidden_dim': 48, 'n1_gat_layers': 3, 'n2_gru_layers': 2, 'dropout': 0.1, 'num_heads': 8, 'k': 5}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_trial)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = study.best_trial.params['hidden_dim']\n",
    "n1_gat_layers = study.best_trial.params['n1_gat_layers']\n",
    "n2_gru_layers = study.best_trial.params['n2_gru_layers']\n",
    "dropout = study.best_trial.params['dropout']\n",
    "num_heads = study.best_trial.params['num_heads']\n",
    "k = study.best_trial.params['k']\n",
    "\n",
    "model = GraphGRUMortalityModel(input_dim=padded_tensor_train.shape[2], hidden_dim=hidden_dim, n1_gat_layers=n1_gat_layers,\n",
    "                            n2_gru_layers=n2_gru_layers, X_core=padd_tensor_core, core_padding_mask=padding_mask_core,\n",
    "                            num_heads=num_heads, dropout=dropout, seq_len=padded_tensor_train.shape[1], k=k, gnn_falg=True).to(DEVICE)\n",
    "\n",
    "\n",
    "model.train_all(dataloaders, datasets, epochs=15, learning_rate=1e-3)\n",
    "print(\"Training completed. Validating on test set...\")\n",
    "\n",
    "acc, auc, pr =  model.validate(dataloaders['test'], datasets['test'])\n",
    "print(f'Test Accuracy in GNN model: {acc:.4f}, AUC: {auc:.4f}, AP: {pr:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphGRUMortalityModel(input_dim=padded_tensor_train.shape[2], hidden_dim=hidden_dim, n1_gat_layers=2, n2_gru_layers=2, X_core=padd_tensor_core, \n",
    "                               core_padding_mask=padding_mask_core, num_heads=4, dropout=0.1, seq_len=padded_tensor_train.shape[1], k=k, gnn_falg=False).to(DEVICE)\n",
    "\n",
    "model.train_all(dataloaders, datasets, epochs=15, learning_rate=1e-3)\n",
    "print(\"Training completed. Validating on test set...\")\n",
    "\n",
    "acc, auc, pr =  model.validate(dataloaders['test'], datasets['test'])\n",
    "print(f'Test Accuracy in GNN model: {acc:.4f}, AUC: {auc:.4f}, AP: {pr:.4f}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ddf7f8e5bd9405fb67d08725d4983e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": "black",
      "description_width": ""
     }
    },
    "56463d5b8bee4a088d7c9d7613fd6e20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "auto"
     }
    },
    "78c007406aa24a9f80d79038bb6e14a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb91ad428b04480f995aced9d968ba18",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1ddf7f8e5bd9405fb67d08725d4983e5",
      "value": 84
     }
    },
    "befffb4f7372406291ce48f9bfcdb07b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56463d5b8bee4a088d7c9d7613fd6e20",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_da6f1034c8da4045a1b7aee2d5f78d5d",
      "value": 100
     }
    },
    "c82cd5cfb39e42af888dc187b5d8a58c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "auto"
     }
    },
    "c94eb62fe73b4171937f354e2c2f06fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": "black",
      "description_width": ""
     }
    },
    "da6f1034c8da4045a1b7aee2d5f78d5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": "black",
      "description_width": ""
     }
    },
    "e6c51c60ee844703a0339b2e1dccc64b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c82cd5cfb39e42af888dc187b5d8a58c",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c94eb62fe73b4171937f354e2c2f06fa",
      "value": 59
     }
    },
    "fb91ad428b04480f995aced9d968ba18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "auto"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
